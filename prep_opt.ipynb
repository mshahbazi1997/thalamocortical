{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "np.set_printoptions(linewidth=200)\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "\n",
    "from time import time\n",
    "\n",
    "from myutils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from ctrnn import CTRNNCell, PlotEachBatch\n",
    "\n",
    "# tf.keras.backend.set_floatx('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.2.0', '0.10.1')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__, tfp.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _real_with_test(x, name='x', tol=1e-6):\n",
    "    imag_norm = tf.linalg.norm(tf.reshape(tf.math.imag(x), -1))\n",
    "    tot_norm = tf.cast(tf.linalg.norm(tf.reshape(x, -1)), tf.float64)\n",
    "    if imag_norm / tot_norm > tol:\n",
    "        tf.print(f'||{name}.imag||/||{name}|| = {imag_norm / tot_norm}')\n",
    "    return tf.math.real(x)\n",
    "\n",
    "from collections.abc import Iterable\n",
    "\n",
    "def iterable(obj):\n",
    "    return isinstance(obj, Iterable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation parameters\n",
    "dt = 0.1\n",
    "n_seconds = 10.0\n",
    "\n",
    "# network parameters\n",
    "tau = 1.0\n",
    "N = 200\n",
    "n_frac = 0.1\n",
    "n = int(round(N * n_frac))\n",
    "\n",
    "# Set the CT and TC weights to the same values as the J weights\n",
    "# sigma_U = 1 / np.sqrt(N)\n",
    "# sigma_V = 1 / np.sqrt(N)\n",
    "\n",
    "# Set the CTC effective weights to the same values as the J weights\n",
    "sigma_U = 1 / (n_frac * N * N)**0.25\n",
    "sigma_V = 1 / (n_frac * N * N)**0.25\n",
    "\n",
    "initial_scale = 0.4\n",
    "fixed_norms = True\n",
    "\n",
    "beta = 0.05\n",
    "test_runs = 100\n",
    "test_runs_to_plot = 3\n",
    "\n",
    "# learning\n",
    "learning_rate = 0.01\n",
    "epochs = 20001\n",
    "print_every = 500\n",
    "split_first = False\n",
    "ext_penalty = 1e5\n",
    "dist_min = 1e-2\n",
    "\n",
    "# random seeds\n",
    "numpy_seed = tf_seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "npr.seed(0)\n",
    "J0 = npr.randn(N, N) / np.sqrt(N)\n",
    "V = initial_scale * npr.randn(n, N) * sigma_V\n",
    "U = initial_scale * npr.randn(N, n) * sigma_U\n",
    "w = npr.randn(N) / np.sqrt(N)\n",
    "\n",
    "Js, Us, Vs, ws = [J0], [U], [V], [w]\n",
    "if split_first:\n",
    "    Js = [J0, J0[:N // 2, :N // 2], J0[N // 2:, N // 2:]]\n",
    "    Us = [U, U[:N // 2], U[N // 2:]]\n",
    "    Vs = [V, V[:, :N // 2], V[:, N // 2:]]\n",
    "    ws = [w, w[:N // 2], w[N // 2:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8f6920ce6944ddebbe74dfc3efc7a99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, len(Js), figsize=[6.4 * len(Js), 4.8])\n",
    "ax = ax if iterable(ax) else np.array([ax])\n",
    "theta = np.linspace(0, 2 * np.pi, 500)\n",
    "\n",
    "for ax_, J_, U_, V_ in zip(ax, Js, Us, Vs):\n",
    "    d, v = np.linalg.eig(J_)\n",
    "    d_, v_ = np.linalg.eig(J_ + U_ @ V_)\n",
    "    ax_.plot(np.cos(theta), np.sin(theta), 'k')\n",
    "    ax_.plot(d.real, d.imag, '.')\n",
    "    ax_.plot(d_.real, d_.imag, 'o', fillstyle='none')\n",
    "    ax_.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reciprocal(x, ep=1e-20):\n",
    "    return x / (x * x + ep)\n",
    "\n",
    "@tf.custom_gradient\n",
    "def myeig(A):\n",
    "    e, v = tf.linalg.eig(A)\n",
    "    def grad(grad_e, grad_v):\n",
    "        f = _reciprocal(e[..., None, :] - e[..., None])\n",
    "        f = tf.linalg.set_diag(f, tf.zeros_like(e))\n",
    "        f = tf.math.conj(f)\n",
    "        vt = tf.linalg.adjoint(v)\n",
    "        vgv = vt @ grad_v\n",
    "        mid = tf.linalg.diag(grad_e) + f * (vgv - vt @ (v * tf.linalg.diag_part(vgv)[..., None, :]))\n",
    "        grad_a = tf.linalg.solve(vt, mid @ vt)\n",
    "        return tf.cast(grad_a, A.dtype)\n",
    "    return (e, v), grad\n",
    "\n",
    "@tf.function\n",
    "def loss_f(U, V, J0, w, tau, beta, eigf=myeig):\n",
    "    N = J0.shape[0]\n",
    "    J = J0 + U @ V\n",
    "    d, r = eigf(J)\n",
    "    L = tf.linalg.inv(tf.linalg.adjoint(r) @ r)\n",
    "    g = (d - 1) / tau\n",
    "    g_real = tf.math.real(g)\n",
    "    G1 = -_reciprocal(g[:, None] + tf.math.conj(g))\n",
    "    G2 = (g[:, None] * tf.math.conj(g)) * G1\n",
    "    wR = tf.linalg.matvec(r, tf.cast(w, tf.complex128), transpose_a=True)\n",
    "    distance_to_margin = tf.linalg.set_diag(tf.abs(d[..., None] - d[..., None, :]) - dist_min, tf.zeros_like(d, dtype=tf.float64))\n",
    "    return tf.math.real(tf.reduce_sum((r @ (L * G1)) * tf.math.conj(r)) / N + beta * tf.reduce_sum(wR * tf.linalg.matvec(L * G2, tf.math.conj(wR)))) \\\n",
    "           + ext_penalty * tf.reduce_sum(tf.where(g_real < 0, tf.zeros_like(g_real), g_real)**2) \\\n",
    "#            + ext_penalty * tf.reduce_sum(tf.where(distance_to_margin > 0, tf.zeros_like(distance_to_margin), distance_to_margin)**2) / 2\n",
    "\n",
    "def _get_U_and_V(param):\n",
    "    U, V = tf.split(param, 2, axis=-1)\n",
    "    if fixed_norms:\n",
    "        U = (U / tf.linalg.norm(U, axis=-2, keepdims=True)) * np.sqrt(N) * sigma_U\n",
    "        V = (V / tf.linalg.norm(V, axis=-2, keepdims=True)) * np.sqrt(N) * sigma_V\n",
    "    else:\n",
    "        U = U / tf.linalg.norm(U, axis=-2, keepdims=True)\n",
    "    V = tf.linalg.matrix_transpose(V)\n",
    "    return U, V\n",
    "\n",
    "def forward_backward(param):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(param)\n",
    "        U, V = _get_U_and_V(param)\n",
    "        loss = loss_f(U, V, J0, w, tau, beta, eigf=myeig)\n",
    "    if not hasattr(forward_backward, 'count') or forward_backward.count % 50 == 0:\n",
    "        if not hasattr(forward_backward, 'count'):\n",
    "            forward_backward.count = 0\n",
    "            forward_backward.time = time()\n",
    "        print(f'iter {forward_backward.count:3d}, time {time() - forward_backward.time:.2f}, device {loss.device}')\n",
    "        forward_backward.time = time()\n",
    "    forward_backward.count += 1\n",
    "    return loss, tape.gradient(loss, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "if split_first:\n",
    "    var = [tf.Variable(np.concatenate((Us[i + 1], Vs[i + 1].T), axis=-1)) for i in range(2)]\n",
    "\n",
    "    losses = []\n",
    "    for J_, w_, var_ in zip(Js[1:], ws[1:], var):\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "        losses_ = np.zeros(epochs)\n",
    "        def stager():\n",
    "            U, V = _get_U_and_V(var_)\n",
    "            loss = loss_f(U, V, J_, w_, tau, beta, eigf=myeig)\n",
    "            if not hasattr(stager, 'count') or stager.count % print_every == 0:\n",
    "                if not hasattr(stager, 'count'):\n",
    "                    stager.count = 0\n",
    "                    stager.time = time()\n",
    "                print(f'iter {stager.count:4d}, time {time() - stager.time:5.2f}, loss {loss:8.5f}, device {loss.device}')\n",
    "                stager.time = time()\n",
    "            losses_[stager.count] = loss.numpy()\n",
    "            stager.count += 1\n",
    "            return loss\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            opt.minimize(stager, [var_])\n",
    "\n",
    "        losses.append(losses_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "if split_first:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=[6.4 * 2, 4.8 * 1])\n",
    "    axes = axes.reshape(-1)\n",
    "\n",
    "    axes[0].plot(losses[0])\n",
    "    axes[0].set_ylim(0.1, 3)\n",
    "\n",
    "    axes[1].plot(losses[1])\n",
    "    axes[1].set_ylim(0.1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "if split_first:\n",
    "    Us_new, Vs_new = [], []\n",
    "    for var_ in var:\n",
    "        U, V = [a.numpy() for a in tf.split(var_, 2, axis=-1)]\n",
    "        Us_new.append(U / np.linalg.norm(U, axis=-2, keepdims=True))\n",
    "        Vs_new.append(V.T)\n",
    "    Us_new = [np.concatenate(Us_new, axis=-2)] + Us_new\n",
    "    Vs_new = [np.concatenate(Vs_new, axis=-1)] + Vs_new\n",
    "else:\n",
    "    Us_new, Vs_new = Us, Vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "if split_first:\n",
    "    norms = []\n",
    "    for J_, U0_, V0_, U_, V_ in zip(Js, Us, Vs, Us_new, Vs_new):\n",
    "        d0_, v0_ = np.linalg.eig(J_ + U0_ @ V0_)\n",
    "        d_, v_ = np.linalg.eig(J_ + U_ @ V_)\n",
    "        c0 = npr.randn(d_.shape[0], test_runs)\n",
    "        t = np.arange(n_seconds / dt) * dt\n",
    "\n",
    "        tmp = v0_ @ (np.exp((d0_[:, None] - 1) * t / tau) * np.linalg.solve(v0_, c0).T[..., None])\n",
    "        assert np.allclose(tmp.imag, 0)\n",
    "        norms0_ = np.linalg.norm(tmp, axis=-2)\n",
    "\n",
    "        tmp = v_ @ (np.exp((d_[:, None] - 1) * t / tau) * np.linalg.solve(v_, c0).T[..., None])\n",
    "        assert np.allclose(tmp.imag, 0)\n",
    "        norms_ = np.linalg.norm(tmp, axis=-2)\n",
    "\n",
    "        norms.append((norms0_, norms_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "if split_first:\n",
    "    fig, axes = plt.subplots(3, 3, figsize=[6.4 * 3, 4.8 * 3])\n",
    "\n",
    "    theta = np.linspace(0, 2 * np.pi, 500)\n",
    "\n",
    "    for i, axes_ in enumerate(axes[:-1]):\n",
    "        for ax, J_, U0_, V0_, U_, V_ in zip(axes_, Js, Us, Vs, Us_new, Vs_new):\n",
    "            d, v = np.linalg.eig(J_)\n",
    "            d0_, v0_ = np.linalg.eig(J_ + U0_ @ V0_)\n",
    "            d_, v_ = np.linalg.eig(J_ + U_ @ V_)\n",
    "\n",
    "            ax.plot(np.cos(theta), np.sin(theta), 'k')\n",
    "            ax.plot(d.real, d.imag, '.')\n",
    "            ax.plot(d0_.real, d0_.imag, 'x')\n",
    "            ax.plot(d_.real, d_.imag, 'o', fillstyle='none')\n",
    "            ax.axis('equal')\n",
    "            if i == 1:\n",
    "                ax.set(xlim=(-1.1, 1.1), ylim=(-1.1, 1.1))\n",
    "\n",
    "    for ax, norms_ in zip(axes[-1], norms):\n",
    "        ax.plot(t, norms_[0][:3].T, color='tab:blue')\n",
    "        ax.plot(t, norms_[1][:3].T, color='tab:orange')\n",
    "    # ax.plot(t, norms.mean(axis=0), linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter    0, time  0.00, loss 40992.25297, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter  125, time 75.08, loss  3.24166, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter  250, time 75.01, loss  2.52192, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter  375, time 74.10, loss  2.21016, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter  500, time 70.79, loss  2.02419, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter  625, time 70.97, loss  1.89637, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter  750, time 70.30, loss  1.80108, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter  875, time 69.79, loss  1.72616, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 1000, time 69.57, loss  1.66496, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 1125, time 71.00, loss  1.61353, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 1250, time 69.56, loss  1.56933, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 1375, time 68.88, loss  1.53066, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 1500, time 68.67, loss  1.49633, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 1625, time 69.20, loss  1.46546, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 1750, time 68.67, loss  1.43742, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 1875, time 68.82, loss  1.41172, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 2000, time 69.38, loss  1.38798, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 2125, time 69.34, loss  1.36590, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 2250, time 71.72, loss  1.34525, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 2375, time 69.74, loss  1.32583, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 2500, time 69.56, loss  1.30748, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 2625, time 69.79, loss  1.29007, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 2750, time 72.67, loss  1.27350, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 2875, time 71.87, loss  1.25767, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 3000, time 70.10, loss  1.24251, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 3125, time 70.55, loss  1.22794, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 3250, time 71.09, loss  1.21392, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 3375, time 70.70, loss  1.20039, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 3500, time 72.31, loss  1.18730, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 3625, time 71.79, loss  1.17463, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 3750, time 70.51, loss  1.16234, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 3875, time 71.68, loss  1.15041, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 4000, time 70.23, loss  1.13881, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 4125, time 69.00, loss  1.12751, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 4250, time 70.12, loss  1.11650, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 4375, time 69.34, loss  1.10577, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 4500, time 68.27, loss  1.09530, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 4625, time 72.94, loss  1.08507, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 4750, time 72.60, loss  1.07508, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 4875, time 72.75, loss  1.06531, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 5000, time 70.99, loss  1.05576, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 5125, time 70.63, loss  1.04641, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 5250, time 71.32, loss  1.03727, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 5375, time 74.23, loss  1.02831, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 5500, time 72.30, loss  1.01953, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 5625, time 69.76, loss  1.01093, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 5750, time 69.82, loss  1.00250, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 5875, time 70.66, loss  0.99424, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 6000, time 75.91, loss  0.98613, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 6125, time 72.91, loss  0.97817, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 6250, time 75.59, loss  0.97036, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 6375, time 73.46, loss  0.96269, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 6500, time 78.91, loss  0.95515, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 6625, time 71.57, loss  0.94775, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 6750, time 72.56, loss  0.94047, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 6875, time 73.81, loss  0.93331, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 7000, time 72.19, loss  0.92626, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 7125, time 73.64, loss  0.91933, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 7250, time 75.09, loss  0.91251, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 7375, time 74.93, loss  0.90579, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 7500, time 71.68, loss  0.89918, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 7625, time 71.23, loss  0.89267, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 7750, time 71.55, loss  0.88626, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 7875, time 72.52, loss  0.87995, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 8000, time 72.41, loss  0.87373, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 8125, time 71.40, loss  0.86761, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 8250, time 73.79, loss  0.86157, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 8375, time 71.72, loss  0.85563, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 8500, time 70.81, loss  0.84977, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 8625, time 69.45, loss  0.84401, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 8750, time 70.49, loss  0.83833, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 8875, time 72.56, loss  0.83274, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 9000, time 73.29, loss  0.82724, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 9125, time 73.03, loss  0.82182, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 9250, time 72.07, loss  0.81648, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 9375, time 71.69, loss  0.81122, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 9500, time 71.28, loss  0.80605, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 9625, time 71.30, loss  0.80095, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 9750, time 72.82, loss  0.79593, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 9875, time 71.30, loss  0.79099, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 10000, time 71.50, loss  0.78613, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 10125, time 71.62, loss  0.78134, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 10250, time 72.24, loss  0.77663, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 10375, time 72.41, loss  0.77198, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 10500, time 72.90, loss  0.76741, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 10625, time 73.44, loss  0.76290, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 10750, time 75.71, loss  0.75846, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 10875, time 70.80, loss  0.75408, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 11000, time 70.10, loss  0.74976, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 11125, time 70.64, loss  0.74551, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 11250, time 70.57, loss  0.74130, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 11375, time 73.33, loss  0.73716, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 11500, time 74.89, loss  0.73307, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 11625, time 75.13, loss  0.72903, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 11750, time 76.24, loss  0.72503, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 11875, time 71.66, loss  0.72108, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 12000, time 70.69, loss  0.71718, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 12125, time 69.89, loss  0.71332, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 12250, time 68.90, loss  0.70950, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 12375, time 68.69, loss  0.70571, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 12500, time 67.98, loss  0.70196, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 12625, time 2196.88, loss  0.69825, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 12750, time 228.15, loss  0.69457, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 12875, time 474.00, loss  0.69092, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 13000, time 73.23, loss  0.68730, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 13125, time 903.32, loss  0.68372, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 13250, time 2528.97, loss  0.68018, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 13375, time 121.25, loss  0.67669, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 13500, time 73.45, loss  0.67327, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 13625, time 2338.74, loss  0.66992, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 13750, time 1283.61, loss  0.66668, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 13875, time 9218.89, loss  0.66355, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 14000, time 4299.27, loss  0.66288, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 14125, time 8194.00, loss  0.65838, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 14250, time 78.71, loss  0.65527, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 14375, time 74.93, loss  0.65252, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 14500, time 74.48, loss  0.64995, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 14625, time 72.70, loss  0.64753, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 14750, time 70.62, loss  0.64523, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 14875, time 72.58, loss  0.64304, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 15000, time 72.92, loss  0.64095, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 15125, time 73.85, loss  0.63896, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 15250, time 72.98, loss  0.63706, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 15375, time 72.07, loss  0.63523, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 15500, time 73.01, loss  0.63349, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 15625, time 72.78, loss  0.63182, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 15750, time 73.44, loss  0.63022, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 15875, time 73.79, loss  0.62868, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 16000, time 72.81, loss  0.62721, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 16125, time 73.71, loss  0.62579, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 16250, time 71.45, loss  0.62442, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 16375, time 72.96, loss  0.62310, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 16500, time 72.20, loss  0.62183, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 16625, time 72.79, loss  0.62060, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 16750, time 72.15, loss  0.61941, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 16875, time 71.78, loss  0.61827, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 17000, time 72.81, loss  0.61717, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 17125, time 72.26, loss  0.61612, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 17250, time 72.59, loss  0.61512, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 17375, time 73.06, loss  0.61416, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 17500, time 72.51, loss  0.61326, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 17625, time 74.10, loss  0.61241, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 17750, time 73.92, loss  0.61162, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 17875, time 73.63, loss  0.61088, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 18000, time 74.01, loss  0.61020, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 18125, time 74.99, loss  0.60957, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 18250, time 73.83, loss  0.60900, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 18375, time 73.40, loss  0.60848, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 18500, time 73.48, loss  0.60800, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 18625, time 73.39, loss  0.60757, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 18750, time 82.16, loss  0.60718, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 18875, time 75.53, loss  0.60683, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 19000, time 73.05, loss  0.60652, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 19125, time 73.68, loss  0.60625, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 19250, time 74.59, loss  0.60600, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 19375, time 74.15, loss  0.60579, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 19500, time 74.99, loss  0.60560, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 19625, time 74.46, loss  0.60544, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 19750, time 75.05, loss  0.60529, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 19875, time 75.91, loss  0.60517, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 20000, time 76.53, loss  0.60506, device /job:localhost/replica:0/task:0/device:CPU:0\n"
     ]
    }
   ],
   "source": [
    "var = tf.Variable(np.concatenate((Us_new[0], Vs_new[0].T), axis=-1))\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "losses = np.zeros(epochs)\n",
    "loss_best = [np.inf]\n",
    "\n",
    "var_best = [tf.zeros_like(var)]\n",
    "\n",
    "def stager():\n",
    "    U, V = _get_U_and_V(var)\n",
    "    loss = loss_f(U, V, Js[0], ws[0], tau, beta, eigf=myeig)\n",
    "    if not hasattr(stager, 'count') or stager.count % (print_every // 4) == 0:\n",
    "        if not hasattr(stager, 'count'):\n",
    "            stager.count = 0\n",
    "            stager.time = time()\n",
    "        print(f'iter {stager.count:4d}, time {time() - stager.time:5.2f}, loss {loss:8.5f}, device {loss.device}')\n",
    "        stager.time = time()\n",
    "    losses[stager.count] = loss.numpy()\n",
    "    if losses[stager.count] < loss_best[0]:\n",
    "        loss_best[0] = losses[stager.count]\n",
    "        var_best[0] = var.numpy()\n",
    "        \n",
    "    stager.count += 1\n",
    "    return loss\n",
    "\n",
    "for _ in range(epochs):\n",
    "    opt.minimize(stager, [var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sean/miniconda3/envs/mypy/lib/python3.7/site-packages/ipykernel_launcher.py:1: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27cb58fbda024ce8bff7d33043db5f82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(0.6050649143810473, (array([20000]),))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.ylim(0.4, 2)\n",
    "plt.yscale('log')\n",
    "loss_best[0], np.nonzero(losses == loss_best[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float64, numpy=0.6050641290637333>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=0.6050649143810473>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_best[0], var\n",
    "loss_f(*_get_U_and_V(var), Js[0], ws[0], tau, beta, eigf=myeig), loss_f(*_get_U_and_V(var_best[0]), Js[0], ws[0], tau, beta, eigf=myeig),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e552a13f1b014134b3e1a0ba3b3ab431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "U_end, V_end = _get_U_and_V(var)\n",
    "\n",
    "d0_, v0_ = np.linalg.eig(Js[0] + Us[0] @ Vs[0])\n",
    "d_, v_ = np.linalg.eig(Js[0] + U_end @ V_end)\n",
    "c0 = npr.randn(d_.shape[0], test_runs)\n",
    "t = np.arange(n_seconds / dt) * dt\n",
    "\n",
    "tmp = v0_ @ (np.exp((d0_[:, None] - 1) * t / tau) * np.linalg.solve(v0_, c0).T[..., None])\n",
    "assert np.allclose(tmp.imag, 0)\n",
    "norms0_ = np.linalg.norm(tmp, axis=-2)\n",
    "\n",
    "tmp = v_ @ (np.exp((d_[:, None] - 1) * t / tau) * np.linalg.solve(v_, c0).T[..., None])\n",
    "assert np.allclose(tmp.imag, 0)\n",
    "norms_ = np.linalg.norm(tmp, axis=-2)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=[6.4 * 2, 4.8 * 2])\n",
    "\n",
    "axes[0, 0].plot(losses)\n",
    "# plt.yscale('log')\n",
    "axes[0, 0].set_ylim(.3, 2)\n",
    "\n",
    "axes[0, 1].plot(t, norms0_[:test_runs_to_plot].T, color='tab:blue')\n",
    "axes[0, 1].plot(t, norms_[:test_runs_to_plot].T, color='tab:orange')\n",
    "axes[0, 1].set_ylim(-1.2, 13.2)\n",
    "# ax.plot(t, norms.mean(axis=0), linewidth=2)\n",
    "\n",
    "theta = np.linspace(0, 2 * np.pi, 500)\n",
    "d, v = np.linalg.eig(Js[0])\n",
    "for i, ax in enumerate(axes[1]):\n",
    "    ax.plot(np.cos(theta), np.sin(theta), 'k')\n",
    "    ax.plot(d.real, d.imag, '.')\n",
    "#     ax.plot(d0_.real, d0_.imag, 'x')\n",
    "    ax.plot(d_.real, d_.imag, 'o', fillstyle='none')\n",
    "    ax.axis('equal')\n",
    "    if i == 1:\n",
    "        ax.set(xlim=(-1.1, 1.1), ylim=(-1.1, 1.1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sean/miniconda3/envs/mypy/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68ec9daac3e146568a594df448f98163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(-0.5, 0.5)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "plt.figure()\n",
    "plt.ylim(0, 10)\n",
    "\n",
    "sns.distplot(Js[0].flatten(), hist = False, kde = True,\n",
    "                 kde_kws = {'linewidth': 2, 'shade': True},\n",
    "                 label = f'J (std = {Js[0].flatten().std():5.3f})')\n",
    "sns.distplot(np.concatenate((U_end, tf.transpose(V_end)), -1).flatten(), hist = False, kde = True,\n",
    "                 kde_kws = {'linewidth': 2, 'shade': True},\n",
    "                 label = f'v & u (std = {np.concatenate((U_end, tf.transpose(V_end)), -1).flatten().std():5.3f})')\n",
    "sns.distplot((U_end @ V_end).numpy().flatten(), hist = False, kde = True,\n",
    "                 kde_kws = {'linewidth': 2, 'shade': True, 'gridsize': 10000},\n",
    "                 label = f\"v'u (std = {(U_end @ V_end).numpy().flatten().std():5.3f})\")\n",
    "plt.xlim(-0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sean/miniconda3/envs/mypy/lib/python3.7/site-packages/ipykernel_launcher.py:1: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8481208102d34eacaf93d15faf01ebfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(-0.5, 0.5)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.ylim(0, 10)\n",
    "\n",
    "sns.distplot(Js[0].flatten(), hist = False, kde = True,\n",
    "                 kde_kws = {'linewidth': 2, 'shade': True},\n",
    "                 label = f'J (std = {Js[0].flatten().std():5.3f})')\n",
    "sns.distplot(np.concatenate((U_end, tf.transpose(V_end)), -1).flatten(), hist = False, kde = True,\n",
    "                 kde_kws = {'linewidth': 2, 'shade': True},\n",
    "                 label = f'v & u (std = {np.concatenate((U_end, tf.transpose(V_end)), -1).flatten().std():5.3f})')\n",
    "sns.distplot((U_end @ V_end).numpy().flatten(), hist = False, kde = True,\n",
    "                 kde_kws = {'linewidth': 2, 'shade': True, 'gridsize': 10000},\n",
    "                 label = f\"v'u (std = {(U_end @ V_end).numpy().flatten().std():5.3f})\")\n",
    "plt.xlim(-0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.044721359549995794"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.14953487812212204, 0.1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 / (n_frac * N * N)**0.25, 1 / N**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 12\n",
    "abstract = 156\n",
    "figure_captions = 1373\n",
    "references = 874\n",
    "intro_and_results = 3565"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "np.set_printoptions(linewidth=200)\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "\n",
    "from time import time\n",
    "\n",
    "from myutils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from ctrnn import CTRNNCell, PlotEachBatch\n",
    "\n",
    "# tf.keras.backend.set_floatx('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.2.0', '0.10.1')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__, tfp.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _real_with_test(x, name='x', tol=1e-6):\n",
    "    imag_norm = tf.linalg.norm(tf.reshape(tf.math.imag(x), -1))\n",
    "    tot_norm = tf.cast(tf.linalg.norm(tf.reshape(x, -1)), tf.float64)\n",
    "    if imag_norm / tot_norm > tol:\n",
    "        tf.print(f'||{name}.imag||/||{name}|| = {imag_norm / tot_norm}')\n",
    "    return tf.math.real(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation parameters\n",
    "dt = 0.1\n",
    "n_seconds = 10.0\n",
    "\n",
    "# network parameters\n",
    "tau = 1.0\n",
    "N = 200\n",
    "n_frac = 0.2\n",
    "beta = 0.05\n",
    "test_runs = 100\n",
    "test_runs_to_plot = 3\n",
    "\n",
    "# learning\n",
    "learning_rate = 0.05\n",
    "epochs = 4001\n",
    "print_every = 200\n",
    "\n",
    "# random seeds\n",
    "numpy_seed = tf_seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "npr.seed(0)\n",
    "n = int(round(N * n_frac))\n",
    "J0 = npr.randn(N, N) / np.sqrt(N)\n",
    "V = 1.0 * npr.randn(n, N) / np.sqrt(N)\n",
    "U = 1.0 * npr.randn(N, n) / np.sqrt(n)\n",
    "w = npr.randn(N) / np.sqrt(N)\n",
    "\n",
    "Js = [J0, J0[:N // 2, :N // 2], J0[N // 2:, N // 2:]]\n",
    "Us = [U, U[:N // 2], U[N // 2:]]\n",
    "Vs = [V, V[:, :N // 2], V[:, N // 2:]]\n",
    "ws = [w, w[:N // 2], w[N // 2:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad00cbeffee1479fa6519751b461291a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=[6.4 * 3, 4.8])\n",
    "theta = np.linspace(0, 2 * np.pi, 500)\n",
    "\n",
    "for ax_, J_, U_, V_ in zip(ax, Js, Us, Vs):\n",
    "    d, v = np.linalg.eig(J_)\n",
    "    d_, v_ = np.linalg.eig(J_ + U_ @ V_)\n",
    "    ax_.plot(np.cos(theta), np.sin(theta), 'k')\n",
    "    ax_.plot(d.real, d.imag, '.')\n",
    "    ax_.plot(d_.real, d_.imag, 'o', fillstyle='none')\n",
    "    ax_.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reciprocal(x, ep=1e-20):\n",
    "    return x / (x * x + ep)\n",
    "\n",
    "@tf.custom_gradient\n",
    "def myeig(A):\n",
    "    e, v = tf.linalg.eig(A)\n",
    "    def grad(grad_e, grad_v):\n",
    "        f = _reciprocal(e[..., None, :] - e[..., None])\n",
    "        f = tf.linalg.set_diag(f, tf.zeros_like(e))\n",
    "        f = tf.math.conj(f)\n",
    "        vt = tf.linalg.adjoint(v)\n",
    "        vgv = vt @ grad_v\n",
    "        mid = tf.linalg.diag(grad_e) + f * (vgv - vt @ (v * tf.linalg.diag_part(vgv)[..., None, :]))\n",
    "        grad_a = tf.linalg.solve(vt, mid @ vt)\n",
    "        return tf.cast(grad_a, A.dtype)\n",
    "    return (e, v), grad\n",
    "\n",
    "@tf.function\n",
    "def loss_f(U, V, J0, w, tau, beta, eigf=myeig):\n",
    "    N = J0.shape[0]\n",
    "    J = J0 + U @ V\n",
    "    d, r = eigf(J)\n",
    "    L = tf.linalg.inv(tf.linalg.adjoint(r) @ r)\n",
    "    g = (d - 1) / tau\n",
    "    G1 = -1 / (g[:, None] + tf.math.conj(g))\n",
    "    G2 = (g[:, None] * tf.math.conj(g)) * G1\n",
    "    wR = tf.linalg.matvec(r, tf.cast(w, tf.complex128), transpose_a=True) \n",
    "    return tf.math.real(tf.reduce_sum((r @ (L * G1)) * tf.math.conj(r)) / N + beta * tf.reduce_sum(wR * tf.linalg.matvec(L * G2, tf.math.conj(wR))))\n",
    "\n",
    "def forward_backward(param):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(param)\n",
    "        U, V = tf.split(param, 2, axis=-1)\n",
    "        U = U / tf.linalg.norm(U, axis=-2, keepdims=True)\n",
    "        V = tf.linalg.matrix_transpose(V)\n",
    "        loss = loss_f(U, V, J0, w, tau, beta, eigf=myeig)\n",
    "    if not hasattr(forward_backward, 'count') or forward_backward.count % 50 == 0:\n",
    "        if not hasattr(forward_backward, 'count'):\n",
    "            forward_backward.count = 0\n",
    "            forward_backward.time = time()\n",
    "        print(f'iter {forward_backward.count:3d}, time {time() - forward_backward.time:.2f}, device {loss.device}')\n",
    "        forward_backward.time = time()\n",
    "    forward_backward.count += 1\n",
    "    return loss, tape.gradient(loss, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter    0, time  0.00, loss  0.85639, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter  200, time 12.64, loss  0.27797, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter  400, time 12.37, loss  0.26842, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter  600, time 11.60, loss  0.26416, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter  800, time 11.65, loss  0.26159, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 1000, time 11.40, loss  0.25986, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 1200, time 11.37, loss  0.25849, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 1400, time 11.20, loss  0.25745, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 1600, time 11.18, loss  0.25660, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 1800, time 11.47, loss  0.25596, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 2000, time 11.86, loss  0.25528, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 2200, time 11.07, loss  0.25475, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 2400, time 11.15, loss  0.25429, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 2600, time 11.37, loss  0.25388, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 2800, time 11.17, loss  0.25349, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 3000, time 11.01, loss  0.25325, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 3200, time 11.52, loss  0.25306, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 3400, time 11.28, loss  0.25258, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 3600, time 11.25, loss  0.25229, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 3800, time 11.45, loss  0.25206, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 4000, time 11.08, loss  0.25192, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter    0, time  0.00, loss  0.93136, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter  200, time 13.56, loss  0.28120, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter  400, time 13.31, loss  0.26953, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter  600, time 12.22, loss  0.26446, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter  800, time 11.90, loss  0.26144, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 1000, time 12.98, loss  0.25936, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 1200, time 12.78, loss  0.25780, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 1400, time 13.40, loss  0.25659, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 1600, time 13.18, loss  0.25579, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 1800, time 11.82, loss  0.25486, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 2000, time 11.73, loss  0.25407, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 2200, time 11.95, loss  0.25355, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 2400, time 11.45, loss  0.25292, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 2600, time 11.45, loss  0.25244, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 2800, time 12.00, loss  0.25201, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 3000, time 11.61, loss  0.25161, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 3200, time 11.46, loss  0.25130, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 3400, time 12.11, loss  0.25092, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 3600, time 11.80, loss  0.25077, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 3800, time 11.32, loss  0.25036, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 4000, time 12.10, loss  0.25013, device /job:localhost/replica:0/task:0/device:CPU:0\n"
     ]
    }
   ],
   "source": [
    "var = [tf.Variable(np.concatenate((Us[i + 1], Vs[i + 1].T), axis=-1)) for i in range(2)]\n",
    "\n",
    "losses = []\n",
    "for J_, w_, var_ in zip(Js[1:], ws[1:], var):\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    losses_ = np.zeros(epochs)\n",
    "    def stager():\n",
    "        U, V = tf.split(var_, 2, axis=-1)\n",
    "        U = U / tf.linalg.norm(U, axis=-2, keepdims=True)\n",
    "        V = tf.linalg.matrix_transpose(V)\n",
    "        loss = loss_f(U, V, J_, w_, tau, beta, eigf=myeig)\n",
    "        if not hasattr(stager, 'count') or stager.count % print_every == 0:\n",
    "            if not hasattr(stager, 'count'):\n",
    "                stager.count = 0\n",
    "                stager.time = time()\n",
    "            print(f'iter {stager.count:4d}, time {time() - stager.time:5.2f}, loss {loss:8.5f}, device {loss.device}')\n",
    "            stager.time = time()\n",
    "        losses_[stager.count] = loss.numpy()\n",
    "        stager.count += 1\n",
    "        return loss\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        opt.minimize(stager, [var_])\n",
    "\n",
    "    losses.append(losses_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51f7c3428af243fb8e2231ceaf5fd944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(0.1, 1.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=[6.4 * 2, 4.8 * 1])\n",
    "axes = axes.reshape(-1)\n",
    "\n",
    "axes[0].plot(losses[0])\n",
    "axes[0].set_ylim(0.1, 1)\n",
    "\n",
    "axes[1].plot(losses[1])\n",
    "axes[1].set_ylim(0.1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Us_new, Vs_new = [], []\n",
    "for var_ in var:\n",
    "    U, V = [a.numpy() for a in tf.split(var_, 2, axis=-1)]\n",
    "    Us_new.append(U / np.linalg.norm(U, axis=-2, keepdims=True))\n",
    "    Vs_new.append(V.T)\n",
    "Us_new = [np.concatenate(Us_new, axis=-2)] + Us_new\n",
    "Vs_new = [np.concatenate(Vs_new, axis=-1)] + Vs_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = []\n",
    "for J_, U0_, V0_, U_, V_ in zip(Js, Us, Vs, Us_new, Vs_new):\n",
    "    d0_, v0_ = np.linalg.eig(J_ + U0_ @ V0_)\n",
    "    d_, v_ = np.linalg.eig(J_ + U_ @ V_)\n",
    "    c0 = npr.randn(d_.shape[0], test_runs)\n",
    "    t = np.arange(n_seconds / dt) * dt\n",
    "\n",
    "    tmp = v0_ @ (np.exp((d0_[:, None] - 1) * t / tau) * np.linalg.solve(v0_, c0).T[..., None])\n",
    "    assert np.allclose(tmp.imag, 0)\n",
    "    norms0_ = np.linalg.norm(tmp, axis=-2)\n",
    "\n",
    "    tmp = v_ @ (np.exp((d_[:, None] - 1) * t / tau) * np.linalg.solve(v_, c0).T[..., None])\n",
    "    assert np.allclose(tmp.imag, 0)\n",
    "    norms_ = np.linalg.norm(tmp, axis=-2)\n",
    "    \n",
    "    norms.append((norms0_, norms_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d22b3379eea44f38527a1330c2cc78c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=[6.4 * 3, 4.8 * 3])\n",
    "\n",
    "theta = np.linspace(0, 2 * np.pi, 500)\n",
    "\n",
    "for i, axes_ in enumerate(axes[:-1]):\n",
    "    for ax, J_, U0_, V0_, U_, V_ in zip(axes_, Js, Us, Vs, Us_new, Vs_new):\n",
    "        d, v = np.linalg.eig(J_)\n",
    "        d0_, v0_ = np.linalg.eig(J_ + U0_ @ V0_)\n",
    "        d_, v_ = np.linalg.eig(J_ + U_ @ V_)\n",
    "\n",
    "        ax.plot(np.cos(theta), np.sin(theta), 'k')\n",
    "        ax.plot(d.real, d.imag, '.')\n",
    "        ax.plot(d0_.real, d0_.imag, 'x')\n",
    "        ax.plot(d_.real, d_.imag, 'o', fillstyle='none')\n",
    "        ax.axis('equal')\n",
    "        if i == 1:\n",
    "            ax.set(xlim=(-1.1, 1.1), ylim=(-1.1, 1.1))\n",
    "\n",
    "for ax, norms_ in zip(axes[-1], norms):\n",
    "    ax.plot(t, norms_[0][:3].T, color='tab:blue')\n",
    "    ax.plot(t, norms_[1][:3].T, color='tab:orange')\n",
    "# ax.plot(t, norms.mean(axis=0), linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter    0, time  0.00, loss  5.74241, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter   50, time 29.23, loss  0.94449, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter  100, time 28.87, loss  0.90660, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter  150, time 28.94, loss  0.83384, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter  200, time 28.62, loss  0.79063, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter  250, time 28.71, loss  0.75856, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter  300, time 28.92, loss  0.73272, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter  350, time 28.24, loss  0.71088, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter  400, time 27.92, loss  0.69184, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter  450, time 27.23, loss  0.67487, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter  500, time 27.17, loss  0.65953, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter  550, time 27.05, loss  0.64555, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter  600, time 28.01, loss  0.63276, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter  650, time 29.60, loss  0.62108, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter  700, time 29.03, loss  0.61044, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter  750, time 29.71, loss  0.60077, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter  800, time 28.74, loss  0.59196, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter  850, time 27.93, loss  0.58391, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter  900, time 27.93, loss  0.57651, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter  950, time 28.60, loss  0.56966, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 1000, time 29.05, loss  0.56330, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 1050, time 29.03, loss  0.55737, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 1100, time 29.25, loss  0.55182, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 1150, time 27.54, loss  0.54661, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 1200, time 27.17, loss  0.54170, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 1250, time 26.62, loss  0.53706, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 1300, time 26.33, loss  0.53269, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 1350, time 27.12, loss  0.52854, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 1400, time 26.65, loss  0.52461, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 1450, time 26.83, loss  0.52088, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 1500, time 27.30, loss  0.51733, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 1550, time 27.30, loss  0.51395, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 1600, time 27.21, loss  0.51074, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 1650, time 27.27, loss  0.50767, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 1700, time 26.69, loss  0.50475, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 1750, time 26.87, loss  0.50196, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 1800, time 26.87, loss  0.49929, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 1850, time 27.87, loss  0.49674, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 1900, time 27.15, loss  0.49429, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 1950, time 26.78, loss  0.49195, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 2000, time 26.60, loss  0.48971, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 2050, time 26.73, loss  0.48755, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 2100, time 26.44, loss  0.48549, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 2150, time 26.36, loss  0.48350, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 2200, time 26.68, loss  0.48159, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 2250, time 26.27, loss  0.47975, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 2300, time 26.61, loss  0.47797, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 2350, time 26.85, loss  0.47627, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 2400, time 27.09, loss  0.47462, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 2450, time 27.20, loss  0.47303, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 2500, time 26.13, loss  0.47150, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 2550, time 26.00, loss  0.47002, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 2600, time 26.72, loss  0.46858, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 2650, time 26.94, loss  0.46720, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 2700, time 26.39, loss  0.46586, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 2750, time 25.95, loss  0.46456, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 2800, time 25.46, loss  0.46331, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 2850, time 25.63, loss  0.46209, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 2900, time 25.26, loss  0.46091, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 2950, time 25.26, loss  0.45976, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 3000, time 25.32, loss  0.45865, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 3050, time 26.16, loss  0.45757, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 3100, time 25.46, loss  0.45652, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 3150, time 25.03, loss  0.45550, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 3200, time 25.14, loss  0.45451, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 3250, time 25.06, loss  0.45355, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 3300, time 24.65, loss  0.45262, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 3350, time 24.64, loss  0.45170, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 3400, time 24.93, loss  0.45082, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 3450, time 24.70, loss  0.44996, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 3500, time 24.06, loss  0.44912, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 3550, time 23.82, loss  0.44830, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 3600, time 24.33, loss  0.44750, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 3650, time 24.36, loss  0.44672, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 3700, time 24.47, loss  0.44597, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 3750, time 24.48, loss  0.44523, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 3800, time 24.13, loss  0.44451, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 3850, time 23.46, loss  0.44380, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 3900, time 23.40, loss  0.44312, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 3950, time 23.24, loss  0.44245, device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "iter 4000, time 23.08, loss  0.44180, device /job:localhost/replica:0/task:0/device:CPU:0\n"
     ]
    }
   ],
   "source": [
    "# var = tf.Variable(np.concatenate((Us_new[0], Vs_new[0].T), axis=-1))\n",
    "var = tf.Variable(np.concatenate((Us[0], Vs[0].T), axis=-1))\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "losses = np.zeros(epochs)\n",
    "\n",
    "def stager():\n",
    "    U, V = tf.split(var, 2, axis=-1)\n",
    "    U = U / tf.linalg.norm(U, axis=-2, keepdims=True)\n",
    "    V = tf.linalg.matrix_transpose(V)\n",
    "    loss = loss_f(U, V, Js[0], ws[0], tau, beta, eigf=myeig)\n",
    "    if not hasattr(stager, 'count') or stager.count % (print_every // 4) == 0:\n",
    "        if not hasattr(stager, 'count'):\n",
    "            stager.count = 0\n",
    "            stager.time = time()\n",
    "        print(f'iter {stager.count:4d}, time {time() - stager.time:5.2f}, loss {loss:8.5f}, device {loss.device}')\n",
    "        stager.time = time()\n",
    "    losses[stager.count] = loss.numpy()\n",
    "    stager.count += 1\n",
    "    return loss\n",
    "\n",
    "for _ in range(epochs):\n",
    "    opt.minimize(stager, [var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ffe403d914484eac03932c25f11e61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "U_end, V_end = [a.numpy() for a in tf.split(var, 2, axis=-1)]\n",
    "U_end = U_end / np.linalg.norm(U_end, axis=-2, keepdims=True)\n",
    "V_end = V_end.T\n",
    "\n",
    "d0_, v0_ = np.linalg.eig(Js[0] + Us[0] @ Vs[0])\n",
    "d_, v_ = np.linalg.eig(Js[0] + U_end @ V_end)\n",
    "c0 = npr.randn(d_.shape[0], test_runs)\n",
    "t = np.arange(n_seconds / dt) * dt\n",
    "\n",
    "tmp = v0_ @ (np.exp((d0_[:, None] - 1) * t / tau) * np.linalg.solve(v0_, c0).T[..., None])\n",
    "assert np.allclose(tmp.imag, 0)\n",
    "norms0_ = np.linalg.norm(tmp, axis=-2)\n",
    "\n",
    "tmp = v_ @ (np.exp((d_[:, None] - 1) * t / tau) * np.linalg.solve(v_, c0).T[..., None])\n",
    "assert np.allclose(tmp.imag, 0)\n",
    "norms_ = np.linalg.norm(tmp, axis=-2)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=[6.4 * 2, 4.8 * 2])\n",
    "\n",
    "axes[0, 0].plot(losses)\n",
    "# plt.yscale('log')\n",
    "axes[0, 0].set_ylim(.3, 2)\n",
    "\n",
    "axes[0, 1].plot(t, norms0_[:3].T, color='tab:blue')\n",
    "axes[0, 1].plot(t, norms_[:3].T, color='tab:orange')\n",
    "axes[0, 1].set_ylim(-2, 22)\n",
    "# ax.plot(t, norms.mean(axis=0), linewidth=2)\n",
    "\n",
    "theta = np.linspace(0, 2 * np.pi, 500)\n",
    "d, v = np.linalg.eig(Js[0])\n",
    "for i, ax in enumerate(axes[1]):\n",
    "    ax.plot(np.cos(theta), np.sin(theta), 'k')\n",
    "    ax.plot(d.real, d.imag, '.')\n",
    "#     ax.plot(d0_.real, d0_.imag, 'x')\n",
    "    ax.plot(d_.real, d_.imag, 'o', fillstyle='none')\n",
    "    ax.axis('equal')\n",
    "    if i == 1:\n",
    "        ax.set(xlim=(-1.1, 1.1), ylim=(-1.1, 1.1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 25\n",
    "reps = 10\n",
    "npr.seed(0)\n",
    "\n",
    "Us = [tf.constant(npr.randn(n, (n * 2) // 5)) for n in np.linspace(N, N * reps, reps, dtype=int)]\n",
    "Vs = [tf.constant(npr.randn((n * 2) // 5, n)) for n in np.linspace(N, N * reps, reps, dtype=int)]\n",
    "As = [tf.constant(npr.randn(n, n) / np.sqrt(n)) for n in np.linspace(N, N * reps, reps, dtype=int)]\n",
    "ws = [tf.constant(npr.randn(n) / np.sqrt(n)) for n in np.linspace(N, N * reps, reps, dtype=int)]\n",
    "tau = 1\n",
    "beta = 0.5\n",
    "\n",
    "curtime = time()\n",
    "systapes = []\n",
    "syslosses = []\n",
    "for i in range(reps):    \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch([Us[i], Vs[i]])\n",
    "        syslosses.append(loss_f(Us[i], Vs[i], As[i], ws[i], tau, beta))\n",
    "    systapes.append(tape)\n",
    "print(time() - curtime)\n",
    "\n",
    "curtime = time()\n",
    "mytapes = []\n",
    "mylosses = []\n",
    "for i in range(reps):    \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch([Us[i], Vs[i]])\n",
    "        mylosses.append(loss_f(Us[i], Vs[i], As[i], ws[i], tau, beta, eigf=myeig))\n",
    "    mytapes.append(tape)\n",
    "print(time() - curtime)\n",
    "\n",
    "curtime = time()\n",
    "sysgrads = []\n",
    "for tape, loss, U, V in zip(systapes, syslosses, Us, Vs):\n",
    "    sysgrads.append(tape.gradient(loss, [U, V]))\n",
    "print(time() - curtime)\n",
    "\n",
    "curtime = time()\n",
    "mygrads = []\n",
    "for tape, loss, U, V in zip(mytapes, mylosses, Us, Vs):\n",
    "    mygrads.append(tape.gradient(loss, [U, V]))\n",
    "print(time() - curtime)\n",
    "\n",
    "for mygrad, sysgrad in zip(mygrads, sysgrads):\n",
    "    print([(np.allclose(a, b), isclose(a, b, 1e-6)) for a, b in zip(mygrad, sysgrad)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
